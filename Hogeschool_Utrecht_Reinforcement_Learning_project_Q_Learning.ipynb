{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hogeschool-Utrecht_Reinforcement-Learning-project_Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eU2QJ--T4dd"
      },
      "source": [
        "# Reinforcement Learning project - Q-Learning\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qdkwbpRUhzO"
      },
      "source": [
        "## Aim\n",
        "In this lab we are going to solve two simple [OpenAI Gym](https://gym.openai.com/) environments using [Q-Learning](https://en.wikipedia.org/wiki/Q-learning). Specifically, the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0/) and [MountainCar-v0](https://gym.openai.com/envs/MountainCar-v0/) environments.\n",
        "\n",
        "We will try to create a table containing the expected reward for each combination of a *state* and *action*. We will use this table to choose the (hopefully) best action given the state the system is in.\n",
        "\n",
        "While this may not be the most advanced or complicated model there is, it is perfect for this task! Furthermore, it can be trained in a relatively short time!\n",
        "\n",
        "## Runtime and environment\n",
        "This [Jupyter Notebook](https://jupyterlab.readthedocs.io/en/latest/) was made to run on Google Colab. For this training, we recommend using the Google Colab environment.\n",
        "\n",
        "Please read the [instructions on Google Colab](https://medium.com/swlh/the-best-place-to-get-started-with-ai-google-colab-tutorial-for-beginners-715e64bb603b) to get started quickly. It behaves similar to Jupyter Notebook, Jupyter Hub and Jupyter Lab, so if you have any experience with those, you're good to go!\n",
        "\n",
        "Some notes on Google Colab:\n",
        "- **Processes in Google Colab won't run forever**. These may be terminated at any time when the platform is crowded, and *will definitely* terminate after 12 hours. To maintain persistency, you can attach the session to **Google Drive** and have your models persist themselves to the Google Drive periodically.\n",
        "- You can enable GPU or TPU support! You can find this option under *Runtime* -> *Change runtime type*.\n",
        "- After installing dependencies, you need to restart the runtime in order to actually use them.\n",
        "\n",
        "If you want to run the code on your own platform or system, you need to keep a few things in mind:\n",
        "- The dependencies you need to install may differ from the ones we installed here. The installed dependencies are suitable for Google Colab, Ubuntu, and Debian.\n",
        "- Since Google Colab isn't attached to a monitor, we render the output to a video file. On your own machine the built-in render method from OpenAI's Gym may suffice.\n",
        "- The default paths use Google Drive! Change these.\n",
        "\n",
        "## Info Support\n",
        "This assignment was developed by Info Support. Looking for a graduation project or job? Check out their website: https://carriere.infosupport.com/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7mfjQyuT_zV"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_LwZ18PXsaL"
      },
      "source": [
        "Some dependencies need to be installed for the code to work. Furthermore, we will define some methods which allow us to show the OpenAI Gym renderings in this (headless) Google Colab environment.\n",
        "\n",
        "You only have to run these and don't need to change any of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBdwK87YUI9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02069a75-50f9-47fc-f6f6-e9eac8465161"
      },
      "source": [
        "# Install dependencies\n",
        "\"\"\"Note: if you are running this code on your own machine, you probably don't need all of these.\n",
        "   Start with 'pip install gym' and install more packages if you run into errors.\"\"\"\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg cmake > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install colabgymrender"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (56.2.0)\n",
            "Requirement already satisfied: colabgymrender in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (2.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (4.1.2.30)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (5.5.0)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay->colabgymrender) (0.3)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.19.5)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.41.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (56.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (5.0.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (2.6.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->colabgymrender) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->colabgymrender) (0.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->colabgymrender) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->colabgymrender) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7IG_wxod9mW"
      },
      "source": [
        "# Imports for helper functions\n",
        "import base64\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from colabgymrender.recorder import Recorder\n",
        "from google.colab import drive\n",
        "from gym.wrappers import Monitor\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFDD54_Afgs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c4fae0-1b53-45a6-f7c8-8263f7608981"
      },
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJeiCQFRgnSj"
      },
      "source": [
        "# Create a directory to store the data for this lab\n",
        "data_path = Path('/content/gdrive/My Drive/Colab Notebooks/HU_RL/part1')\n",
        "data_path.mkdir(parents=True, exist_ok=True)\n",
        "video_path = data_path / 'video'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU2ZsnBEelqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5acbf4-65f4-4746-ace1-a16af4db46bf"
      },
      "source": [
        "# Define helper functions to visually show what the models are doing.\n",
        "%matplotlib inline\n",
        "\n",
        "gym.logger.set_level(gym.logger.ERROR)\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    # Display the stored video file\n",
        "    # Credits: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "    mp4list = list(data_path.glob('video/*.mp4'))\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[-1]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print('Could not find video')\n",
        "\n",
        "\n",
        "def record_episode(idx):\n",
        "    # This determines which episodes to record.\n",
        "    # Since the video rendering in the OpenAI Gym is a bit buggy, we simply override it and decide\n",
        "    # whether or not to render inside of our training loop.\n",
        "    return True\n",
        "\n",
        "    \n",
        "def video_env(env):\n",
        "    # Wraps the environment to write its output to a video file\n",
        "    env = Monitor(env, video_path, video_callable=record_episode, force=True)\n",
        "    return env\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f8147797210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShMPjUFXiwli"
      },
      "source": [
        "# Test the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3hAfAGhi4KK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "5df41deb-d8c3-434d-fd01-646e898a39a3"
      },
      "source": [
        "\"\"\"We will use a basic OpenAI Gym examle: CartPole-v0.\n",
        "In this example, we will try to balance a pole on a cart.\n",
        "This is similar to kids (and.. grown-ups) trying to balance sticks on their hands.\n",
        "\n",
        "Check out the OpenAI Gym documentation to learn more: https://gym.openai.com/docs/\"\"\"\n",
        "\n",
        "# Create the desired environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Wrap the environment, to make sure we get to see a fancy video\n",
        "env = video_env(env)\n",
        "\n",
        "# Before you can use a Gym environment, it needs to be reset.\n",
        "state = env.reset()\n",
        "\n",
        "# Perform random actions untill we drop the stick. Just as an example.\n",
        "done = False\n",
        "while not done:\n",
        "    env.render()\n",
        "    # The action_space contains all possible actions we can take.\n",
        "    random_action = env.action_space.sample() \n",
        "\n",
        "    # After each action, we end up in a new state and receive a reward.\n",
        "    # When we drop the pole (more than 12 degrees), or balance it long enough (200 steps),\n",
        "    # or drive off the screen, done is set to True.\n",
        "    state, reward, done, info = env.step(random_action)\n",
        "\n",
        "# Show the results!\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We will use a basic OpenAI Gym examle: CartPole-v0.\\nIn this example, we will try to balance a pole on a cart.\\nThis is similar to kids (and.. grown-ups) trying to balance sticks on their hands.\\n\\nCheck out the OpenAI Gym documentation to learn more: https://gym.openai.com/docs/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "            </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-O3zaP6q4-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838c59b8-74a2-4640-e8d2-d459b1aa2ad3"
      },
      "source": [
        "# Neat, it did something (randomly)! \n",
        "\n",
        "# In order to train the system, we will try to predict the reward a certain actions yields given the state of the system.\n",
        "# But what is the state anyway?\n",
        "\n",
        "# In this environment, the state represents the cart's position and velocity, and the pole's angle and velocity.\n",
        "\n",
        "# Let's check out the current state\n",
        "print(f'Cart position: {state[0]} (range: [-4.8, 4.8])')\n",
        "print(f'Cart velocity: {state[1]} (range: [-inf, inf])')\n",
        "print(f'Pole angle: {state[2]} (range: [-0.418, 0.418])')\n",
        "print(f'Pole velocity: {state[3]} (range [-inf, inf])')\n",
        "\n",
        "# You can find out the minimum and maximum possible observation values using:\n",
        "print(f'Low observation space:', env.observation_space.low)\n",
        "print(f'High observation space:', env.observation_space.high)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cart position: 0.11821278935927293 (range: [-4.8, 4.8])\n",
            "Cart velocity: 0.9540942707315812 (range: [-inf, inf])\n",
            "Pole angle: -0.22903012229116515 (range: [-0.418, 0.418])\n",
            "Pole velocity: -1.6206608198265988 (range [-inf, inf])\n",
            "Low observation space: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
            "High observation space: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhQlBt2hUAQ2"
      },
      "source": [
        "# Implement Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFsyJOo5GrZe"
      },
      "source": [
        "## Task\n",
        "Try to find proper values for the **?** parameters and reach a 200 reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDvyHdRyRtdq"
      },
      "source": [
        "# Define parameters - Fill in the dots\n",
        "num_episodes = 25_000\n",
        "num_episodes_between_status_update = 1000\n",
        "num_episodes_between_videos = 5000\n",
        "\n",
        "learning_rate = 0.03       # also known as: alpha\n",
        "discount = 0.95            # also known as: gamma\n",
        "epsilon = 0.7\n",
        "\n",
        "# Epsilon decay\n",
        "epsilon_decay = .9999\n",
        "epsilon_low = 1e-2\n",
        "\n",
        "# Discretization\n",
        "num_buckets_per_observation = 20\n",
        "\n",
        "obs_space_low = env.observation_space.low\n",
        "obs_space_low[1] = -5.0\n",
        "obs_space_low[3] = -5.0\n",
        "\n",
        "obs_space_high = env.observation_space.high\n",
        "obs_space_high[1] = 5\n",
        "obs_space_high[3] = 5\n",
        "\n",
        "observation_range = obs_space_high - obs_space_low"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0S8rYCWT15s"
      },
      "source": [
        "## Q-Table creation\n",
        "\n",
        "# As seen before, the state consists of 4 floating point values.\n",
        "# It makes sense to discretize these values (read: place them in buckets), to reduce the state space and therefore the Q-table size\n",
        "bucket_shape = [num_buckets_per_observation] * len(env.observation_space.high)\n",
        "bucket_size = observation_range / bucket_shape\n",
        "\n",
        "# Define the initial Q table as a random uniform distribution\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(bucket_shape + [env.action_space.n]))\n",
        "\n",
        "#print('Initial Q table:', q_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hlrqLKohWPD"
      },
      "source": [
        "# Functions\n",
        "\n",
        "def discretize_state(state):\n",
        "    discrete_state = (state - obs_space_low) / bucket_size\n",
        "    return tuple(discrete_state.astype(np.int))\n",
        "\n",
        "def take_action(discrete_state, epsilon):\n",
        "    # Take an action to either explore or exploit.\n",
        "    if np.random.random() > epsilon:\n",
        "        return np.argmax(q_table[discrete_state])\n",
        "    return env.action_space.sample()\n",
        "\n",
        "def estimated_max_for_next_state(discrete_state):\n",
        "    # What's the best expected Q-value for the next state?\n",
        "    estimated_max = np.max(q_table[discrete_state])\n",
        "    return estimated_max\n",
        "\n",
        "def new_q_value(discrete_state, action, max_future_q):\n",
        "    # Calculate the new Q-value\n",
        "    current_q = q_table[discrete_state + (action,)]\n",
        "    new_q = (1 - learning_rate) * current_q + learning_rate * (reward + discount * max_future_q)\n",
        "    return new_q\n",
        "\n",
        "def decayed_epsilon(epsilon):\n",
        "    # Optionally, decay the epsilon value\n",
        "    if epsilon > epsilon_low:\n",
        "      return epsilon * epsilon_decay\n",
        "    return epsilon_low"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Cp8cNtUcDf"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMUc3G164cA7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc008d3c-d381-489d-8c12-ac61e0860932"
      },
      "source": [
        "# Time to train the system\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset() # Don't forget to reset the environment between episodes\n",
        "    current_state_disc = discretize_state(state)\n",
        "\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = take_action(current_state_disc, epsilon)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        new_state_disc = discretize_state(new_state)\n",
        "\n",
        "        reward_sum += reward\n",
        "\n",
        "        if not done:\n",
        "            # Retrieve the maximum estimated value for the next state\n",
        "            max_future_q = estimated_max_for_next_state(new_state_disc)\n",
        "            \n",
        "            # Calculate the new value (note: Bellman equation)\n",
        "            new_q = new_q_value(current_state_disc, action, max_future_q)\n",
        "            q_table[current_state_disc + (action,)] = new_q\n",
        "        else:\n",
        "            # Render the video\n",
        "            if (episode + 1) % num_episodes_between_status_update == 0:\n",
        "                env.render()\n",
        "                print(f'Total reward at episode {episode + 1}: {reward_sum}')\n",
        "                print(epsilon)\n",
        "\n",
        "        # Prepare for the next loop\n",
        "        current_state_disc = new_state_disc\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = decayed_epsilon(epsilon)\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 1000: 26.0\n",
            "0.633446370128003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 2000: 63.0\n",
            "0.5731631119970844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 3000: 34.0\n",
            "0.5186168371093486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 4000: 56.0\n",
            "0.46926157336983776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 5000: 54.0\n",
            "0.42460330726807105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 6000: 80.0\n",
            "0.38419503912990177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 7000: 200.0\n",
            "0.34763230894675157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 8000: 200.0\n",
            "0.3145491479987312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 9000: 200.0\n",
            "0.28461441575006985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 10000: 200.0\n",
            "0.2575284853516138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 11000: 200.0\n",
            "0.23302024457445247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 12000: 200.0\n",
            "0.21084438215602447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 13000: 107.0\n",
            "0.19077893239680196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 14000: 153.0\n",
            "0.17262305343061046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 15000: 200.0\n",
            "0.15619501693053253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 16000: 200.0\n",
            "0.14133038912868123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 17000: 200.0\n",
            "0.1278803849430616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 18000: 200.0\n",
            "0.11571037873741241\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 19000: 200.0\n",
            "0.10469855680772938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 20000: 200.0\n",
            "0.09473469810774295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 21000: 84.0\n",
            "0.08571907100922514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 22000: 200.0\n",
            "0.07756143505442856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 23000: 200.0\n",
            "0.07018013770885281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 24000: 182.0\n",
            "0.06350129707343911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 25000: 200.0\n",
            "0.057458062375681455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "            </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YwiptrMhnDV"
      },
      "source": [
        "# MountainCar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rnoKeLMhi5C"
      },
      "source": [
        "Now apply the things you've learned to the MountainCar problem. Please note that the observable space differs from the previous problem. Thus, before you start training, you need to learn more about thethis new environment.\n",
        "\n",
        "Here is some code to help you get started.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g97BXKeZ0a9d"
      },
      "source": [
        "# Create the desired environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "# env = video_env(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC0mlpZe0XBG"
      },
      "source": [
        "# Define parameters - Fill in the dots\n",
        "num_episodes = 10_000\n",
        "num_episodes_between_status_update = 1000\n",
        "num_episodes_between_videos = 5000\n",
        "\n",
        "learning_rate = 0.075      # also known as: alpha\n",
        "discount = 0.95            # also known as: gamma\n",
        "epsilon = 1\n",
        "\n",
        "# Epsilon decay\n",
        "epsilon_decay = .999\n",
        "epsilon_low = 1e-2\n",
        "\n",
        "# Discretization\n",
        "num_buckets_per_observation = 20\n",
        "\n",
        "obs_space_low = env.observation_space.low\n",
        "# obs_space_low[1] = -5.0\n",
        "# obs_space_low[3] = -5.0\n",
        "\n",
        "obs_space_high = env.observation_space.high\n",
        "# obs_space_high[1] = 5\n",
        "# obs_space_high[3] = 5\n",
        "\n",
        "observation_range = obs_space_high - obs_space_low"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbF2bVnh08Qs"
      },
      "source": [
        "## Q-Table creation\n",
        "\n",
        "# As seen before, the state consists of 4 floating point values.\n",
        "# It makes sense to discretize these values (read: place them in buckets), to reduce the state space and therefore the Q-table size\n",
        "bucket_shape = [num_buckets_per_observation] * len(env.observation_space.high)\n",
        "bucket_size = observation_range / bucket_shape\n",
        "\n",
        "# Define the initial Q table as a random uniform distribution\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(bucket_shape + [env.action_space.n]))\n",
        "\n",
        "#print('Initial Q table:', q_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_npVhOIdpZn"
      },
      "source": [
        "# Time to train the system\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset() # Don't forget to reset the environment between episodes\n",
        "    current_state_disc = discretize_state(state)\n",
        "\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "    for _ in range(200):\n",
        "        action = take_action(current_state_disc, epsilon)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        new_state_disc = discretize_state(new_state)\n",
        "\n",
        "        reward_sum += reward\n",
        "\n",
        "        if not done:\n",
        "            # Retrieve the maximum estimated value for the next state\n",
        "            max_future_q = estimated_max_for_next_state(new_state_disc)\n",
        "            \n",
        "            # Calculate the new value (note: Bellman equation)\n",
        "            new_q = new_q_value(current_state_disc, action, max_future_q)\n",
        "            q_table[current_state_disc + (action,)] = new_q\n",
        "        else:\n",
        "            # Render the video\n",
        "            if (episode + 1) % num_episodes_between_status_update == 0:\n",
        "                env.render()\n",
        "                print(f'Total reward at episode {episode + 1}: {reward_sum}')\n",
        "                print(epsilon)\n",
        "\n",
        "        # Prepare for the next loop\n",
        "        current_state_disc = new_state_disc\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = decayed_epsilon(epsilon)\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpIdmPAe1REo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}